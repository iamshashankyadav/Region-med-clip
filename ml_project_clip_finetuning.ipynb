{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bBToMZZM8axl"
      },
      "outputs": [],
      "source": [
        "# Cell 1: install required packages\n",
        "!pip install -q sentence-transformers transformers pillow tqdm pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: imports & config\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------------------\n",
        "# EDIT THESE PATHS TO MATCH YOUR FILES IF NEEDED\n",
        "# ---------------------------\n",
        "IMG_DIR = \"/content/images\"  # full/global images\n",
        "ROI_DIR = \"/content/roi_images\"  # if you have ROI crops (optional)\n",
        "GLOBAL_CSV = \"/content/lesion_metadata_with_global_labels.csv\"   # should contain columns: image_id,global_text\n",
        "LOCAL_JSON = \"/content/local_labels_final.csv\"   # mapping image_id -> {\"roi_imgs\":[...],\"roi_texts\":[...]} (optional)\n",
        "TEACHER_GLOBAL_NPZ = \"/content/biomedclip_global_embeddings.npz\"  # teacher fused global embeddings (fused_embeds, image_ids)\n",
        "TEACHER_LOCAL_NPY = \"/content/biomedclip_local_embeddings.npz\"  # object-array of per-image arrays of local teacher embeddings (optional)\n",
        "TEST72_CSV = \"/content/eval_sample.csv\"  # test split (72). If not present notebook will create it.\n",
        "OUT_DIR = \"/content/output\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Paths set. Make sure the files exist at these locations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puE-mqtDBkkO",
        "outputId": "8b2e4db1-11ff-4aea-b49c-c4b531961e4a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Paths set. Make sure the files exist at these locations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: check that critical files exist (warn but don't crash)\n",
        "def warn_path(p):\n",
        "    if not os.path.exists(p):\n",
        "        print(\"MISSING:\", p)\n",
        "    else:\n",
        "        print(\"FOUND:\", p)\n",
        "\n",
        "warn_path(IMG_DIR)\n",
        "warn_path(GLOBAL_CSV)\n",
        "warn_path(TEACHER_GLOBAL_NPZ)\n",
        "# local files optional\n",
        "warn_path(LOCAL_JSON)\n",
        "warn_path(TEACHER_LOCAL_NPY)\n",
        "warn_path(TEST72_CSV)\n",
        "\n",
        "print(\"If any required file is missing, upload it to the path above or edit the path variables in Cell 2.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuUS2wAzRCtL",
        "outputId": "d6a2cdf0-2f34-4f9b-e4d4-b84354408476"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOUND: /content/images\n",
            "FOUND: /content/lesion_metadata_with_global_labels.csv\n",
            "FOUND: /content/biomedclip_global_embeddings.npz\n",
            "FOUND: /content/local_labels_final.csv\n",
            "FOUND: /content/biomedclip_local_embeddings.npz\n",
            "FOUND: /content/eval_sample.csv\n",
            "If any required file is missing, upload it to the path above or edit the path variables in Cell 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.load(\"biomedclip_global_embeddings.npz\").files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb52D_93eJXL",
        "outputId": "3c782aea-c693-4370-8e79-106ec4fcbf34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image_embeds', 'text_embeds', 'image_ids']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.load(\"biomedclip_local_embeddings.npz\").files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPxrtNG-ff2x",
        "outputId": "c49e6598-0989-47f2-de3b-6cc49a153a6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image_embeds', 'text_embeds', 'image_ids']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch, numpy as np, pandas as pd, os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "student = SentenceTransformer(\"sentence-transformers/clip-ViT-B-32\", device=device)\n",
        "print(\"Student model loaded. Embedding dim:\", student.get_sentence_embedding_dimension())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Te-6sDfksE",
        "outputId": "5588f34d-cc3a-4c59-f804-c6cd8a113548"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Student model loaded. Embedding dim: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df1=pd.read_csv(\"/content/eval_sample.csv\")\n",
        "# df1[\"image_id\"]+=\".jpg\"\n",
        "# df1.to_csv(\"/content/eval_sample.csv\")"
      ],
      "metadata": {
        "id": "UX24Ix69kQj5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2\n",
        "def encode_images_student(student_model, pil_images, batch_size=16):\n",
        "    if len(pil_images) == 0:\n",
        "        return torch.zeros((0, student_model.get_sentence_embedding_dimension()), device=device)\n",
        "    emb = student_model.encode(pil_images, batch_size=batch_size, convert_to_tensor=True, device=device)\n",
        "    return emb  # already on device\n",
        "\n",
        "def encode_texts_student(student_model, texts, batch_size=64):\n",
        "    if len(texts) == 0:\n",
        "        return torch.zeros((0, student_model.get_sentence_embedding_dimension()), device=device)\n",
        "    emb = student_model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=device)\n",
        "    return emb\n"
      ],
      "metadata": {
        "id": "rWSc1ObeiTRw"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - set your path variables (these were already set earlier in your environment)\n",
        "# If you already have these variables defined, this will overwrite - adjust paths if needed.\n",
        "IMG_DIR = \"/content/images\"\n",
        "GLOBAL_CSV = \"/content/lesion_metadata_with_global_labels.csv\"\n",
        "TEST72_CSV = \"/content/eval_sample.csv\"\n",
        "OUT_DIR = \"/content/output\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load or create test72\n",
        "if not os.path.exists(TEST72_CSV):\n",
        "    df_all = pd.read_csv(GLOBAL_CSV)\n",
        "    np.random.seed(42)\n",
        "    idx = np.random.choice(len(df_all), 72, replace=False)\n",
        "    df_test72 = df_all.iloc[idx].reset_index(drop=True)\n",
        "    df_test72.to_csv(TEST72_CSV, index=False)\n",
        "    print(\"Created test72:\", TEST72_CSV)\n",
        "else:\n",
        "    df_test72 = pd.read_csv(TEST72_CSV)\n",
        "    print(\"Loaded existing test72:\", TEST72_CSV)\n",
        "\n",
        "# Encode baseline embeddings for test72\n",
        "test_imgs = []\n",
        "test_texts = []\n",
        "for _, r in df_test72.iterrows():\n",
        "    p = os.path.join(IMG_DIR, str(r['image_id']))\n",
        "    test_imgs.append(Image.open(p).convert(\"RGB\"))\n",
        "    test_texts.append(str(r['global_label']))\n",
        "\n",
        "print(\"Encoding test72 images and texts...\")\n",
        "img_embs = encode_images_student(student, test_imgs, batch_size=16)  # tensor\n",
        "txt_embs = encode_texts_student(student, test_texts, batch_size=64)\n",
        "\n",
        "img_np = img_embs.cpu().numpy()\n",
        "txt_np = txt_embs.cpu().numpy()\n",
        "sim = img_np @ txt_np.T\n",
        "\n",
        "np.savez(os.path.join(OUT_DIR, \"baseline_test72_student.npz\"),\n",
        "         image_embeds=img_np, text_embeds=txt_np, sim=sim,\n",
        "         image_ids=df_test72['image_id'].values, texts=df_test72['global_label'].values)\n",
        "print(\"Saved baseline artifacts to\", os.path.join(OUT_DIR, \"baseline_test72_student.npz\"))\n",
        "\n",
        "# quick unsupervised checks\n",
        "pos = np.diag(sim)\n",
        "neg = sim[~np.eye(len(sim), dtype=bool)]\n",
        "print(\"Avg pos sim:\", pos.mean(), \"Avg neg sim:\", neg.mean())\n",
        "\n",
        "def recall_at_k(sim_matrix, k):\n",
        "    ranks = np.argsort(-sim_matrix, axis=1)\n",
        "    hits = sum(i in ranks[i, :k] for i in range(sim_matrix.shape[0]))\n",
        "    return hits / sim_matrix.shape[0]\n",
        "\n",
        "for k in (1,5,10):\n",
        "    print(f\"Recall@{k}:\", recall_at_k(sim, k))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux9v3gZGifcQ",
        "outputId": "8a3277ff-b3a7-47ae-8344-1299c5fa3622"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing test72: /content/eval_sample.csv\n",
            "Encoding test72 images and texts...\n",
            "Saved baseline artifacts to /content/output/baseline_test72_student.npz\n",
            "Avg pos sim: 26.11878 Avg neg sim: 26.099163\n",
            "Recall@1: 0.0\n",
            "Recall@5: 0.041666666666666664\n",
            "Recall@10: 0.1527777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4\n",
        "TEACHER_GLOBAL_NPZ = \"/content/biomedclip_global_embeddings.npz\"   # adjust if different\n",
        "TEACHER_LOCAL_NPZ  = \"/content/biomedclip_local_embeddings.npz\"    # adjust if different (you said you have both NPZ)\n",
        "\n",
        "tg = np.load(TEACHER_GLOBAL_NPZ, allow_pickle=True)\n",
        "teacher_global_embeds = tg['image_embeds']  # shape (N, D)\n",
        "teacher_global_text_embeds = tg.get('text_embeds', None)\n",
        "teacher_global_ids = [str(x) for x in tg['image_ids']]\n",
        "\n",
        "print(\"Teacher global loaded:\", teacher_global_embeds.shape)\n",
        "\n",
        "# Local teacher NPZ\n",
        "if os.path.exists(TEACHER_LOCAL_NPZ):\n",
        "    tl = np.load(TEACHER_LOCAL_NPZ, allow_pickle=True)\n",
        "    teacher_local_embeds = tl['image_embeds']   # shape (N_local, D) OR (N, D) depending on how saved\n",
        "    teacher_local_text_embeds = tl.get('text_embeds', None)\n",
        "    teacher_local_ids = [str(x) for x in tl['image_ids']]\n",
        "    print(\"Teacher local loaded:\", teacher_local_embeds.shape)\n",
        "else:\n",
        "    teacher_local_embeds = None\n",
        "    teacher_local_text_embeds = None\n",
        "    teacher_local_ids = None\n",
        "    print(\"No teacher local NPZ found at\", TEACHER_LOCAL_NPZ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpoUd3MpioR3",
        "outputId": "30b8e2f7-787a-4b86-b360-627b4f8ef510"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher global loaded: (472, 512)\n",
            "Teacher local loaded: (472, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# CELL: NORMALIZE IDS + DEBUG + SAFE MERGE\n",
        "# ===============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n=== STARTING ID NORMALIZATION ===\")\n",
        "\n",
        "# -------------------------\n",
        "# 1. CLEAN TRAIN DF\n",
        "# -------------------------\n",
        "df_all['image_id'] = df_all['image_id'].astype(str)\n",
        "# Fill NaN labels safely\n",
        "df_all['global_labels'] = df_all['global_label'].astype(str).fillna(\"\")\n",
        "\n",
        "# Remove .jpg/.png\n",
        "df_all['image_id_norm'] = (\n",
        "    df_all['image_id']\n",
        "    .str.replace(\".jpg\", \"\", regex=False)\n",
        "    .str.replace(\".jpeg\", \"\", regex=False)\n",
        "    .str.replace(\".png\", \"\", regex=False)\n",
        ")\n",
        "\n",
        "print(\"Sample normalized train_df IDs:\", df_all['image_id_norm'].head().tolist())\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2. CLEAN TEACHER GLOBAL IDS\n",
        "# -------------------------\n",
        "teacher_ids_raw = list(teacher_global_ids)  # original list\n",
        "teacher_ids_norm = []\n",
        "\n",
        "valid_teacher_idx = []\n",
        "\n",
        "for i, tid in enumerate(teacher_ids_raw):\n",
        "    tid = str(tid)\n",
        "    # Remove extensions if any\n",
        "    tid = tid.replace(\".jpg\", \"\").replace(\".jpeg\", \"\").replace(\".png\", \"\")\n",
        "\n",
        "    # Remove stray whitespace\n",
        "    tid = tid.strip()\n",
        "\n",
        "    # ISIC IDs should start with ISIC_\n",
        "    if not tid.startswith(\"ISIC_\"):\n",
        "        continue\n",
        "\n",
        "    # Many ISIC IDs have length 12 (e.g., ISIC_0027139)\n",
        "    # Some NPZs truncate last 1â€“2 digits (e.g., ISIC_00288)\n",
        "    # We DO NOT DROP THESE â€” instead we allow len >= 8\n",
        "    if len(tid) >= 8:\n",
        "        teacher_ids_norm.append(tid)\n",
        "        valid_teacher_idx.append(i)\n",
        "\n",
        "teacher_global_embeds = teacher_global_embeds[valid_teacher_idx]\n",
        "if teacher_local_embeds is not None:\n",
        "    teacher_local_embeds = teacher_local_embeds[valid_teacher_idx]\n",
        "\n",
        "print(\"Teacher embeddings kept:\", len(teacher_ids_norm))\n",
        "print(\"Sample normalized teacher IDs:\", teacher_ids_norm[:10])\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 3. BUILD ID â†’ INDEX MAP\n",
        "# -------------------------\n",
        "id_to_idx = {tid: i for i, tid in enumerate(teacher_ids_norm)}\n",
        "\n",
        "print(\"Mapping size:\", len(id_to_idx))\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 4. DEBUG: Check how many training IDs match teachers\n",
        "# -------------------------\n",
        "train_ids = df_all['image_id_norm'].tolist()\n",
        "\n",
        "missing = [imgid for imgid in train_ids if imgid not in id_to_idx]\n",
        "\n",
        "print(\"\\n=== MATCHING REPORT ===\")\n",
        "print(\"Total train images:\", len(train_ids))\n",
        "print(\"Train images found in teacher NPZ:\", len(train_ids) - len(missing))\n",
        "print(\"Train images NOT found:\", len(missing))\n",
        "\n",
        "# Show first few missing for debugging\n",
        "print(\"Missing examples:\", missing[:10])\n",
        "\n",
        "# We'll use this mapping inside Dataset later\n",
        "TEACHER_ID_TO_IDX = id_to_idx\n",
        "TEACHER_IDS_NORM = teacher_ids_norm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krr1aE1Pr5a6",
        "outputId": "46985b1a-0181-4539-83cd-63e04c8f9ae2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STARTING ID NORMALIZATION ===\n",
            "Sample normalized train_df IDs: ['ISIC_0027828', 'ISIC_0029161', 'ISIC_0025819', 'ISIC_0027960', 'ISIC_0025140']\n",
            "Teacher embeddings kept: 472\n",
            "Sample normalized teacher IDs: ['ISIC_0027828', 'ISIC_0029161', 'ISIC_0025819', 'ISIC_0027960', 'ISIC_0025140', 'ISIC_0024635', 'ISIC_0025063', 'ISIC_0027957', 'ISIC_0025136', 'ISIC_0027652']\n",
            "Mapping size: 472\n",
            "\n",
            "=== MATCHING REPORT ===\n",
            "Total train images: 472\n",
            "Train images found in teacher NPZ: 472\n",
            "Train images NOT found: 0\n",
            "Missing examples: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5\n",
        "LOCAL_CSV = \"/content/local_labels_final.csv\"  # local labels per image; columns: image_id, local_text, (optional) roi_fname\n",
        "ROI_DIR = \"/content/roi_images\"          # if roi images exist; else we'll reuse global images as ROI placeholders\n",
        "\n",
        "# load global CSV and exclude test72\n",
        "df_all = pd.read_csv(GLOBAL_CSV)\n",
        "test_ids = set(df_test72['image_id'].astype(str).tolist())\n",
        "train_df = df_all[~df_all['image_id'].astype(str).isin(test_ids)].reset_index(drop=True)\n",
        "print(\"Train rows:\", len(train_df))\n",
        "\n",
        "# load local CSV if exists\n",
        "local_map = {}\n",
        "if os.path.exists(LOCAL_CSV):\n",
        "    df_local = pd.read_csv(LOCAL_CSV)\n",
        "    # Expect each row to have image_id and local_text (and optional roi_fname)\n",
        "    for _, r in df_local.iterrows():\n",
        "        imgid = str(r['image_id'])\n",
        "        text = str(r.get('local_text', \"\"))\n",
        "        roi = r.get('roi_fname', None)\n",
        "        local_map[imgid] = {\"roi_imgs\": [roi] if (roi and os.path.exists(os.path.join(ROI_DIR, roi))) else [], \"roi_texts\":[text]}\n",
        "    print(\"Loaded local CSV entries:\", len(local_map))\n",
        "else:\n",
        "    print(\"Local CSV not found at\", LOCAL_CSV, \"- local captions will be empty.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB8xwxtJjGnc",
        "outputId": "5eb66579-f982-4199-b50d-bcba64dca7fa"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 400\n",
            "Loaded local CSV entries: 472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# CELL 6 â€” DEFINITELY WORKING\n",
        "# ===========================\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "EMB_DIM = student.get_sentence_embedding_dimension()\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, df, local_map, teacher_global_embeds, teacher_local_embeds, img_dir, roi_dir, max_rois=1):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.local_map = local_map\n",
        "        self.teacher_global = teacher_global_embeds\n",
        "        self.teacher_local = teacher_local_embeds\n",
        "        self.img_dir = img_dir\n",
        "        self.roi_dir = roi_dir\n",
        "        self.max_rois = max_rois\n",
        "\n",
        "        # Build mapping: teacher ID â†’ teacher index\n",
        "        self.id_to_idx = {}\n",
        "        for i, iid in enumerate(teacher_global_ids_norm):\n",
        "            key = str(iid)\n",
        "            self.id_to_idx[key] = i\n",
        "            key = key.replace(\".jpg\", \"\")  # remove extension if present\n",
        "            key = key.replace(\".png\", \"\")\n",
        "            self.id_to_idx[key] = i\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        # Always use normalized id\n",
        "        img_id_norm = str(row['image_id_norm'])\n",
        "\n",
        "        t_idx = TEACHER_ID_TO_IDX.get(img_id_norm, None)\n",
        "\n",
        "\n",
        "\n",
        "        # Load global image\n",
        "        gpath = os.path.join(self.img_dir, img_id)\n",
        "        gpil = Image.open(gpath).convert(\"RGB\")\n",
        "        gtext = str(row['global_label'])\n",
        "\n",
        "        # Local (ROI)\n",
        "        lm = self.local_map.get(img_id, {\"roi_imgs\": [], \"roi_texts\": []})\n",
        "        roi_imgs = lm[\"roi_imgs\"][:self.max_rois]\n",
        "        roi_texts = lm[\"roi_texts\"][:self.max_rois]\n",
        "\n",
        "        roi_pils = []\n",
        "        for r in roi_imgs:\n",
        "            if r and os.path.exists(os.path.join(self.roi_dir, r)):\n",
        "                roi_pils.append(Image.open(os.path.join(self.roi_dir, r)).convert(\"RGB\"))\n",
        "            else:\n",
        "                roi_pils.append(gpil)\n",
        "\n",
        "        # Pad to max_rois\n",
        "        while len(roi_pils) < self.max_rois:\n",
        "            roi_pils.append(gpil)\n",
        "            roi_texts.append(\"\")\n",
        "\n",
        "        # Teacher embeddings\n",
        "        img_id_norm = str(row['image_id_norm'])\n",
        "        t_idx = TEACHER_ID_TO_IDX.get(img_id_norm, None)\n",
        "\n",
        "        # If teacher not found, always return safe zeros\n",
        "        if t_idx is None:\n",
        "            tg = np.zeros((EMB_DIM,), dtype=np.float32)\n",
        "            tl = np.zeros((self.max_rois, EMB_DIM), dtype=np.float32)\n",
        "        else:\n",
        "            tg = self.teacher_global[t_idx].astype(np.float32)\n",
        "\n",
        "            if self.teacher_local is not None and len(self.teacher_local) == len(self.teacher_global):\n",
        "                raw_local = self.teacher_local[t_idx].astype(np.float32)\n",
        "                if raw_local.ndim == 1:\n",
        "                    raw_local = np.expand_dims(raw_local, 0)\n",
        "                tl = np.zeros((self.max_rois, EMB_DIM), dtype=np.float32)\n",
        "                for i in range(min(self.max_rois, raw_local.shape[0])):\n",
        "                    tl[i] = raw_local[i]\n",
        "            else:\n",
        "                tl = np.zeros((self.max_rois, EMB_DIM), dtype=np.float32)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"global_img\": gpil,\n",
        "            \"global_text\": gtext,\n",
        "            \"roi_imgs\": roi_pils,\n",
        "            \"roi_texts\": roi_texts,\n",
        "            \"teacher_global\": tg,\n",
        "            \"teacher_local\": tl,\n",
        "        }\n",
        "\n",
        "# ðŸ”¥ THIS IS THE MOST IMPORTANT LINE\n",
        "# num_workers MUST BE 0\n",
        "train_dataset = TrainDataset(\n",
        "    train_df, local_map,\n",
        "    teacher_global_embeds,\n",
        "    teacher_local_embeds,\n",
        "    IMG_DIR, ROI_DIR,\n",
        "    max_rois=1\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=0,    # <-- THIS FIXES EVERYTHING\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Train loader ready:\", len(train_loader), \"batches\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "424qOnr0jJHm",
        "outputId": "3e108ed0-7abe-447e-cada-4fe7a63ee6ca"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader ready: 50 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loader object:\", train_loader)\n",
        "print(\"num_workers:\", train_loader.num_workers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDht9DO_nsDo",
        "outputId": "60b90a89-d5d9-4f7d-cd6f-085d33e69839"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader object: <torch.utils.data.dataloader.DataLoader object at 0x7ea11efdc830>\n",
            "num_workers: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def info_nce_loss(img_feats, txt_feats, temp=0.07):\n",
        "    logits = (img_feats @ txt_feats.T) / temp\n",
        "    labels = torch.arange(img_feats.size(0), device=img_feats.device)\n",
        "    loss_i = F.cross_entropy(logits, labels)\n",
        "    loss_t = F.cross_entropy(logits.T, labels)\n",
        "    return 0.5 * (loss_i + loss_t)\n",
        "\n",
        "def roi_contrastive_loss(roi_img_feats, roi_txt_feats, temp=0.07):\n",
        "    # roi_img_feats, roi_txt_feats: B x R x D\n",
        "    B, R, D = roi_img_feats.shape\n",
        "    loss = 0.0\n",
        "    for r in range(R):\n",
        "        loss += info_nce_loss(roi_img_feats[:, r, :], roi_txt_feats[:, r, :], temp=temp)\n",
        "    return loss / max(1, R)\n",
        "\n",
        "def negative_caption_loss(roi_img_feats, roi_txt_feats, temp=0.07):\n",
        "    B, R, D = roi_img_feats.shape\n",
        "    flat_txt = roi_txt_feats.view(B*R, D)\n",
        "    loss = 0.0\n",
        "    for r in range(R):\n",
        "        logits = (roi_img_feats[:, r, :] @ flat_txt.T) / temp  # B x (B*R)\n",
        "        labels = torch.arange(B, device=roi_img_feats.device) * R + r\n",
        "        loss += F.cross_entropy(logits, labels)\n",
        "    return loss / max(1, R)\n",
        "\n",
        "def distill_mse_loss(student_feats, teacher_feats):\n",
        "    return F.mse_loss(student_feats, teacher_feats)\n"
      ],
      "metadata": {
        "id": "iEPoAYUJjOar"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8\n",
        "student.train()\n",
        "optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)\n",
        "alpha, beta, gamma, lam = 1.0, 1.0, 1.0, 0.4\n",
        "\n",
        "print(\"Starting dry-run (10 batches max) to validate pipeline...\")\n",
        "max_batches = 10\n",
        "for bidx, batch in enumerate(train_loader):\n",
        "    if bidx >= max_batches:\n",
        "        break\n",
        "\n",
        "    # global encodings\n",
        "    pil_imgs = list(batch['global_img'])\n",
        "    g_img_emb = encode_images_student(student, pil_imgs)\n",
        "    g_txt_emb = encode_texts_student(student, list(batch['global_labels']))\n",
        "\n",
        "    # ROI encodings (max_rois=1 here so flattening simple)\n",
        "    B = g_img_emb.size(0)\n",
        "    R = len(batch['roi_imgs'][0])\n",
        "    flat_rois = []\n",
        "    for i in range(B):\n",
        "        flat_rois.extend(batch['roi_imgs'][i])\n",
        "    roi_img_flat = encode_images_student(student, flat_rois).view(B, R, -1)\n",
        "    flat_roi_texts = []\n",
        "    for i in range(B):\n",
        "        flat_roi_texts.extend(batch['roi_texts'][i])\n",
        "    roi_txt_flat = encode_texts_student(student, flat_roi_texts).view(B, R, -1)\n",
        "\n",
        "    # teacher arrays\n",
        "    tg = torch.from_numpy(np.vstack(batch['teacher_global'])).to(device).float()\n",
        "    tl = torch.from_numpy(np.stack(batch['teacher_local'])).to(device).float()  # B x R x D\n",
        "\n",
        "    # normalize\n",
        "    g_img_emb = F.normalize(g_img_emb, dim=-1)\n",
        "    g_txt_emb = F.normalize(g_txt_emb, dim=-1)\n",
        "    roi_img_flat = F.normalize(roi_img_flat, dim=-1)\n",
        "    roi_txt_flat = F.normalize(roi_txt_flat, dim=-1)\n",
        "\n",
        "    # compute losses\n",
        "    Lg = info_nce_loss(g_img_emb, g_txt_emb)\n",
        "    Lr = roi_contrastive_loss(roi_img_flat, roi_txt_flat)\n",
        "    Ln = negative_caption_loss(roi_img_flat, roi_txt_flat)\n",
        "    LdG = distill_mse_loss(g_img_emb, tg.to(device))\n",
        "    LdR = distill_mse_loss(roi_img_flat.view(B*R, -1), tl.view(B*R, -1).to(device))\n",
        "    loss = alpha*Lg + beta*Lr + gamma*Ln + lam*(LdG + LdR)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Batch {bidx+1} loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Dry-run done. If no errors, proceed to full training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "DA67PrHujQq9",
        "outputId": "c338524f-9298-402a-d588-b8084c4767a9"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dry-run (10 batches max) to validate pipeline...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'image_id_norm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_id_norm'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1647524979.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting dry-run (10 batches max) to validate pipeline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmax_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbidx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2620796494.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Always use normalized id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimg_id_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEACHER_ID_TO_IDX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_id_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_id_norm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_dataset\n",
        "del train_loader\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "CFJ_qb_WjTBw"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = []\n",
        "for i in train_df['image_id'].astype(str):\n",
        "    if i not in teacher_global_ids:\n",
        "        missing.append(i)\n",
        "len(missing), missing[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3juNOQT0nGte",
        "outputId": "89538dba-9e47-4fa2-bf01-99aa0b3bbbd4"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400,\n",
              " ['ISIC_0029161.jpg',\n",
              "  'ISIC_0025819.jpg',\n",
              "  'ISIC_0027960.jpg',\n",
              "  'ISIC_0025140.jpg',\n",
              "  'ISIC_0024635.jpg',\n",
              "  'ISIC_0025063.jpg',\n",
              "  'ISIC_0027957.jpg',\n",
              "  'ISIC_0025136.jpg',\n",
              "  'ISIC_0027218.jpg',\n",
              "  'ISIC_0027139.jpg'])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_global_ids[:20]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-siQkkIBp4-l",
        "outputId": "0177c5f6-9cbe-4d0f-d1eb-e7b3f874a0d5"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ISIC_0027828',\n",
              " 'ISIC_0029161',\n",
              " 'ISIC_0025819',\n",
              " 'ISIC_0027960',\n",
              " 'ISIC_0025140',\n",
              " 'ISIC_0024635',\n",
              " 'ISIC_0025063',\n",
              " 'ISIC_0027957',\n",
              " 'ISIC_0025136',\n",
              " 'ISIC_0027652',\n",
              " 'ISIC_0027218',\n",
              " 'ISIC_0026298',\n",
              " 'ISIC_0027139',\n",
              " 'ISIC_0027739',\n",
              " 'ISIC_0027781',\n",
              " 'ISIC_0024890',\n",
              " 'ISIC_0025838',\n",
              " 'ISIC_0025485',\n",
              " 'ISIC_0025842',\n",
              " 'ISIC_0028856']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfzEt1m0qD32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}